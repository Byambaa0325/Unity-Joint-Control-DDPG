
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Report.ipynb
from unityagents import UnityEnvironment
import numpy as np

# select this option to load version 1 (with a single agent) of the environment
#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')

# select this option to load version 2 (with 20 agents) of the environment
env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')

# get the default brain
brain_name = env.brain_names[0]
brain = env.brains[brain_name]

import torch
from collections import deque
from ddpg_agent import Agent

env_info = env.reset(train_mode=True)[brain_name]
num_agents = len(env_info.agents)
action_size = brain.vector_action_space_size
states = env_info.vector_observations
state_size = states.shape[1]

def ddpg(n_episodes=500, max_t=1000, start_steps = 10, learn_frequency = 20, learn_count = 10, random_seed = 1):
    """Deep Deterministic Policy Gradient (DDPG)

    Params
    ======
        n_episodes (int)      : maximum number of training episodes
        max_t (int)           : maximum number of timesteps per episode
        start_steps (int)     : number of starting steps actions are chosen randomly
        learn_frequency (int) : frequency of learning per timestep
        learn_count (int)     : number of learning steps to do at learning timestep
        random_seed (int)     : random seed for agent's weights
    """

    agent = Agent(state_size=state_size, action_size=action_size, random_seed=random_seed)   #Initialize the Agent

    avg_scores_episode = []                    # list containing scores from each episode
    avg_scores_moving = []                     # list containing avg scores from window at each episode
    scores_window = deque(maxlen=100)          # last 100 scores

    for i_episode in range(1, n_episodes+1):
        env_info = env.reset(train_mode=True)[brain_name]       # reset environment
        states = env_info.vector_observations                   # get current state for each agent
        scores = np.zeros(num_agents)                           # initialize score for each agent
        agent.reset()                                           # reset noise of the agent

        for t in range(max_t):
            #Randomly sample actions during the starting steps
            if i_episode <= start_steps:
                actions = np.random.randn(num_agents, action_size) # select an action randomly
                actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1
            else:
                actions = agent.act(states, add_noise=True)     # select an action according to policy (for each agent)
            env_info = env.step(actions)[brain_name]            # send actions to environment
            next_states = env_info.vector_observations          # get next state (for each agent)
            rewards = env_info.rewards                          # get reward (for each agent)
            dones = env_info.local_done                         # see if episode has finished (for each agent)

            # for each agent's experience, save it and learn
            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):
                if t % learn_frequency == 0: # Learn with frequency
                    agent.step(state, action, reward, next_state, done, learn = True, learn_count = learn_count)
                else:
                    agent.step(state, action, reward, next_state, done, learn = False) #just add, don't learn

            states = next_states

            scores += rewards                                   # add the rewards from the timestep to the scores
            if np.any(dones):                                   # finish episode if any agent has reached a terminal state
                break


        scores_window.append(np.mean(scores))            # save the most recent score to scores window

        avg_scores_episode.append(np.mean(scores))       # save the most recent score to avg_scores
        avg_scores_moving.append(np.mean(scores_window)) # save the most recent score window average to moving averages


        print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end="")
        if i_episode % 1 == 0: # Print every episode
            print('\rEpisode {}\tAverage Score: {:.2f} \t Current Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)))

        #environment is solved
        if np.mean(scores_window)>=30.0:
            print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))
            torch.save(agent.actor_local.state_dict(), "checkpoint_actor.pth")        #Save actors' weights
            torch.save(agent.critic_local.state_dict(), "checkpoint_critic.pth")      #Save critics' weights
            break

    return avg_scores_episode, avg_scores_moving # Return average score of each episode and moving average at that time

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--n_episodes', type=int, default=500)
    parser.add_argument('--max_t', type=int, default=1000)
    parser.add_argument('--start_steps', type=int, default=20)
    parser.add_argument('--learn_frequency', type=int, default=20)
    parser.add_argument('--learn_count', type=int, default=10)
    parser.add_argument('--random_seed', type=int, default=1)
    args = parser.parse_args()

    ddpg(n_episodes = args.n_episodes, max_t = args.max_t, start_steps = args.start_steps, \
         learn_frequency = args.learn_frequency, learn_count = args.learn_count, random_seed = args.random_seed)
env.close()
