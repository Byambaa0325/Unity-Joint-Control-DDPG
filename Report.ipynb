{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation using Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "This project creates an agent to navigate (and collect bananas!) in a large, square world.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "**Unity Machine Learning Agents (ML-Agents)** is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. It has myriads of environments up for a challenge, and in which the environment we will be solving during this project is **Reacher**.\n",
    "![image.png](https://video.udacity-data.com/topher/2018/June/5b1ea778_reacher/reacher.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing all the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is provided and can be accessed as below once installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents, representing the agent. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is considered <b>solved</b>, when the average (over 100 episodes) of those average scores is at least +30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DDPG Agent Solver\n",
    "\n",
    "The environment is continuous environment which has state space and action space that are both continuous. As such, we would need an agent that can deal with continuous setting. Here **DDPG** algorithm comes to the rescue. Unlike our **DQN** algorithm that is a Q-learning variant which uses deep neural network to approximate the Q-function, **DDPG** also learns policy at the same time as learning Q-function. Therefore, it is a mix between Q-learning and Policy Gradient methods.\n",
    "\n",
    "![image.png](https://spinningup.openai.com/en/latest/_images/math/5811066e89799e65be299ec407846103fcf1f746.svg)\n",
    "\n",
    "The reason for choosing **DDPG** is because it is, in a nutshell, **DQN** for continuous action spaces. If you recall, **DQN** deals with environments with discrete action spaces like pressing buttons while **DDPG** can tell how hard to press that button. As same as **DQN**, it uses: \n",
    "\n",
    "- Replay Buffers\n",
    "- Target Networks\n",
    "\n",
    "The main addition is using calculating the max over actions in the target:\n",
    "![equationDDPG.png](https://spinningup.openai.com/en/latest/_images/math/4421120861d55302d76c7e2fd7cc5b2da7aea320.svg)\n",
    "Computing the maximum over actions in the target is a challenge in continuous action spaces. DDPG deals with this by using a target policy network to compute an action which approximately maximizes the Q network.\n",
    "\n",
    "While I won't go deep down to the details DDPG algorithm, I'll guide you to great resources that does a much better job at explaining it than I would here :)\n",
    "\n",
    "- [The original paper introducing the DDPG](https://arxiv.org/abs/1509.02971)\n",
    "- [DDPG implementation by Spinning Up, OpenAI](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation Details\n",
    "\n",
    "The **DDPG** agent that we will use to solve the environment will be a modified **DDPG** algorithm.\n",
    "\n",
    "#### Hyperparaments\n",
    "Note that the hyper parameters used for the training was:\n",
    "    - n_episodes = 500\n",
    "        Train for total of 500 episodes until the environment is solved\n",
    "    - max_t = 1000 \n",
    "        Maximum of 1000 timesteps per episode\n",
    "    - start_steps = 10\n",
    "        Randomly sample actions for 10 episodes to encourage exploration at the start of the learning process\n",
    "    - learn_frequency = 20\n",
    "        Learn per 20 timesteps when training.\n",
    "    - learn_count = 10\n",
    "        Learn 10 times at the learning step\n",
    "        \n",
    "#### Neural network inside the agent\n",
    "\n",
    "The **Actor** neural network inside the agent is simple shallow network with 3 linear layers with a batch norm within them.\n",
    "\n",
    "    State value -> \n",
    "    LinearLayer(state size, 400) -> Relu ->\n",
    "    BatchNorm1d(400) -> \n",
    "    LinearLayer(400, 300) -> Relu ->\n",
    "    BatchNorm1d(400) -> \n",
    "    LinearLayer(300, action size) ->\n",
    "    Action values\n",
    "\n",
    "The **Critic** neural network inside the agent is simple shallow network with 3 linear layers with a batch norm within them.\n",
    "    \n",
    "    State value -> \n",
    "    Linear(state_size, 400) -> Relu ->\n",
    "    BatchNorm1d(400) -> \n",
    "    Linear(400+action_size, 300) -> Relu ->\n",
    "    Linear(300, 1) -> \n",
    "    Q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implementation\n",
    "\n",
    "Import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from collections import deque\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DDPG** learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ddpg(n_episodes=500, max_t=1000, start_steps = 10, learn_frequency = 20, learn_count = 10, random_seed = 1):\n",
    "    \"\"\"Deep Deterministic Policy Gradient (DDPG)\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int)      : maximum number of training episodes\n",
    "        max_t (int)           : maximum number of timesteps per episode\n",
    "        start_steps (int)     : number of starting steps actions are chosen randomly \n",
    "        learn_frequency (int) : frequency of learning per timestep\n",
    "        learn_count (int)     : number of learning steps to do at learning timestep\n",
    "        random_seed (int)     : random seed for agent's weights \n",
    "    \"\"\"\n",
    "        \n",
    "    agent = Agent(state_size=state_size, action_size=action_size, random_seed=random_seed)   #Initialize the Agent\n",
    "    \n",
    "    avg_scores_episode = []                    # list containing scores from each episode\n",
    "    avg_scores_moving = []                     # list containing avg scores from window at each episode\n",
    "    scores_window = deque(maxlen=100)          # last 100 scores                    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]       # reset environment\n",
    "        states = env_info.vector_observations                   # get current state for each agent      \n",
    "        scores = np.zeros(num_agents)                           # initialize score for each agent\n",
    "        agent.reset()                                           # reset noise of the agent\n",
    "\n",
    "        for t in range(max_t):\n",
    "            #Randomly sample actions during the starting steps\n",
    "            if i_episode <= start_steps:\n",
    "                actions = np.random.randn(num_agents, action_size) # select an action randomly\n",
    "                actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            else:\n",
    "                actions = agent.act(states, add_noise=True)     # select an action according to policy (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]            # send actions to environment\n",
    "            next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "            rewards = env_info.rewards                          # get reward (for each agent)\n",
    "            dones = env_info.local_done                         # see if episode has finished (for each agent)\n",
    "            \n",
    "            # for each agent's experience, save it and learn\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                if t % learn_frequency == 0: # Learn with frequency\n",
    "                    agent.step(state, action, reward, next_state, done, learn = True, learn_count = learn_count)\n",
    "                else:\n",
    "                    agent.step(state, action, reward, next_state, done, learn = False) #just add, don't learn\n",
    "                    \n",
    "            states = next_states\n",
    "            \n",
    "            scores += rewards                                   # add the rewards from the timestep to the scores\n",
    "            if np.any(dones):                                   # finish episode if any agent has reached a terminal state\n",
    "                break\n",
    "\n",
    "                \n",
    "        scores_window.append(np.mean(scores))            # save the most recent score to scores window\n",
    "        \n",
    "        avg_scores_episode.append(np.mean(scores))       # save the most recent score to avg_scores\n",
    "        avg_scores_moving.append(np.mean(scores_window)) # save the most recent score window average to moving averages\n",
    "\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 1 == 0: # Print every episode\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} \\t Current Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)))\n",
    "            \n",
    "        #environment is solved\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), \"checkpoint_actor.pth\")        #Save actors' weights\n",
    "            torch.save(agent.critic_local.state_dict(), \"checkpoint_critic.pth\")      #Save critics' weights\n",
    "            break\n",
    "            \n",
    "    return avg_scores_episode, avg_scores_moving # Return average score of each episode and moving average at that time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent until it solves the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.04 \t Current Score: 0.04\n",
      "Episode 2\tAverage Score: 0.05 \t Current Score: 0.05\n",
      "Episode 3\tAverage Score: 0.07 \t Current Score: 0.11\n",
      "Episode 4\tAverage Score: 0.08 \t Current Score: 0.13\n",
      "Episode 5\tAverage Score: 0.10 \t Current Score: 0.15\n",
      "Episode 6\tAverage Score: 0.11 \t Current Score: 0.18\n",
      "Episode 7\tAverage Score: 0.11 \t Current Score: 0.12\n",
      "Episode 8\tAverage Score: 0.13 \t Current Score: 0.24\n",
      "Episode 9\tAverage Score: 0.15 \t Current Score: 0.29\n",
      "Episode 10\tAverage Score: 0.14 \t Current Score: 0.09\n",
      "Episode 11\tAverage Score: 0.14 \t Current Score: 0.10\n",
      "Episode 12\tAverage Score: 0.13 \t Current Score: 0.10\n",
      "Episode 13\tAverage Score: 0.13 \t Current Score: 0.14\n",
      "Episode 14\tAverage Score: 0.14 \t Current Score: 0.16\n",
      "Episode 15\tAverage Score: 0.13 \t Current Score: 0.05\n",
      "Episode 16\tAverage Score: 0.13 \t Current Score: 0.12\n",
      "Episode 17\tAverage Score: 0.13 \t Current Score: 0.15\n",
      "Episode 18\tAverage Score: 0.13 \t Current Score: 0.11\n",
      "Episode 19\tAverage Score: 0.13 \t Current Score: 0.05\n",
      "Episode 20\tAverage Score: 0.12 \t Current Score: 0.08\n",
      "Episode 21\tAverage Score: 0.16 \t Current Score: 0.95\n",
      "Episode 22\tAverage Score: 0.19 \t Current Score: 0.86\n",
      "Episode 23\tAverage Score: 0.23 \t Current Score: 1.07\n",
      "Episode 24\tAverage Score: 0.27 \t Current Score: 1.17\n",
      "Episode 25\tAverage Score: 0.32 \t Current Score: 1.48\n",
      "Episode 26\tAverage Score: 0.38 \t Current Score: 1.81\n",
      "Episode 27\tAverage Score: 0.45 \t Current Score: 2.27\n",
      "Episode 28\tAverage Score: 0.52 \t Current Score: 2.50\n",
      "Episode 29\tAverage Score: 0.62 \t Current Score: 3.47\n",
      "Episode 30\tAverage Score: 0.69 \t Current Score: 2.67\n",
      "Episode 31\tAverage Score: 0.81 \t Current Score: 4.26\n",
      "Episode 32\tAverage Score: 0.95 \t Current Score: 5.32\n",
      "Episode 33\tAverage Score: 1.12 \t Current Score: 6.70\n",
      "Episode 34\tAverage Score: 1.29 \t Current Score: 6.98\n",
      "Episode 35\tAverage Score: 1.46 \t Current Score: 7.26\n",
      "Episode 36\tAverage Score: 1.62 \t Current Score: 7.16\n",
      "Episode 37\tAverage Score: 1.83 \t Current Score: 9.36\n",
      "Episode 38\tAverage Score: 2.07 \t Current Score: 10.93\n",
      "Episode 39\tAverage Score: 2.26 \t Current Score: 9.55\n",
      "Episode 40\tAverage Score: 2.47 \t Current Score: 10.69\n",
      "Episode 41\tAverage Score: 2.65 \t Current Score: 9.72\n",
      "Episode 42\tAverage Score: 2.84 \t Current Score: 10.78\n",
      "Episode 43\tAverage Score: 3.07 \t Current Score: 12.37\n",
      "Episode 44\tAverage Score: 3.30 \t Current Score: 13.60\n",
      "Episode 45\tAverage Score: 3.53 \t Current Score: 13.52\n",
      "Episode 46\tAverage Score: 3.78 \t Current Score: 14.74\n",
      "Episode 47\tAverage Score: 4.01 \t Current Score: 14.86\n",
      "Episode 48\tAverage Score: 4.28 \t Current Score: 16.69\n",
      "Episode 49\tAverage Score: 4.53 \t Current Score: 16.86\n",
      "Episode 50\tAverage Score: 4.81 \t Current Score: 18.36\n",
      "Episode 51\tAverage Score: 5.05 \t Current Score: 17.10\n",
      "Episode 52\tAverage Score: 5.34 \t Current Score: 20.27\n",
      "Episode 53\tAverage Score: 5.65 \t Current Score: 21.69\n",
      "Episode 54\tAverage Score: 5.95 \t Current Score: 21.85\n",
      "Episode 55\tAverage Score: 6.26 \t Current Score: 23.20\n",
      "Episode 56\tAverage Score: 6.60 \t Current Score: 25.09\n",
      "Episode 57\tAverage Score: 6.91 \t Current Score: 24.02\n",
      "Episode 58\tAverage Score: 7.24 \t Current Score: 26.53\n",
      "Episode 59\tAverage Score: 7.61 \t Current Score: 28.67\n",
      "Episode 60\tAverage Score: 7.95 \t Current Score: 28.15\n",
      "Episode 61\tAverage Score: 8.27 \t Current Score: 27.24\n",
      "Episode 62\tAverage Score: 8.60 \t Current Score: 28.91\n",
      "Episode 63\tAverage Score: 8.94 \t Current Score: 30.14\n",
      "Episode 64\tAverage Score: 9.27 \t Current Score: 29.68\n",
      "Episode 65\tAverage Score: 9.61 \t Current Score: 31.90\n",
      "Episode 66\tAverage Score: 9.96 \t Current Score: 32.19\n",
      "Episode 67\tAverage Score: 10.29 \t Current Score: 32.66\n",
      "Episode 68\tAverage Score: 10.65 \t Current Score: 34.40\n",
      "Episode 69\tAverage Score: 11.02 \t Current Score: 36.09\n",
      "Episode 70\tAverage Score: 11.39 \t Current Score: 36.93\n",
      "Episode 71\tAverage Score: 11.75 \t Current Score: 37.09\n",
      "Episode 72\tAverage Score: 12.08 \t Current Score: 35.56\n",
      "Episode 73\tAverage Score: 12.42 \t Current Score: 36.61\n",
      "Episode 74\tAverage Score: 12.74 \t Current Score: 36.15\n",
      "Episode 75\tAverage Score: 13.02 \t Current Score: 33.92\n",
      "Episode 76\tAverage Score: 13.33 \t Current Score: 36.51\n",
      "Episode 77\tAverage Score: 13.63 \t Current Score: 36.55\n",
      "Episode 78\tAverage Score: 13.93 \t Current Score: 37.14\n",
      "Episode 79\tAverage Score: 14.23 \t Current Score: 37.35\n",
      "Episode 80\tAverage Score: 14.51 \t Current Score: 37.02\n",
      "Episode 81\tAverage Score: 14.79 \t Current Score: 37.15\n",
      "Episode 82\tAverage Score: 15.07 \t Current Score: 37.13\n",
      "Episode 83\tAverage Score: 15.34 \t Current Score: 37.49\n",
      "Episode 84\tAverage Score: 15.60 \t Current Score: 37.44\n",
      "Episode 85\tAverage Score: 15.86 \t Current Score: 37.77\n",
      "Episode 86\tAverage Score: 16.11 \t Current Score: 37.50\n",
      "Episode 87\tAverage Score: 16.35 \t Current Score: 36.79\n",
      "Episode 88\tAverage Score: 16.58 \t Current Score: 36.86\n",
      "Episode 89\tAverage Score: 16.82 \t Current Score: 37.68\n",
      "Episode 90\tAverage Score: 17.05 \t Current Score: 37.49\n",
      "Episode 91\tAverage Score: 17.28 \t Current Score: 37.79\n",
      "Episode 92\tAverage Score: 17.50 \t Current Score: 37.72\n",
      "Episode 93\tAverage Score: 17.70 \t Current Score: 36.42\n",
      "Episode 94\tAverage Score: 17.91 \t Current Score: 36.77\n",
      "Episode 95\tAverage Score: 18.11 \t Current Score: 37.02\n",
      "Episode 96\tAverage Score: 18.31 \t Current Score: 38.08\n",
      "Episode 97\tAverage Score: 18.50 \t Current Score: 36.26\n",
      "Episode 98\tAverage Score: 18.69 \t Current Score: 37.51\n",
      "Episode 99\tAverage Score: 18.89 \t Current Score: 37.75\n",
      "Episode 100\tAverage Score: 19.06 \t Current Score: 36.30\n",
      "Episode 101\tAverage Score: 19.43 \t Current Score: 36.93\n",
      "Episode 102\tAverage Score: 19.80 \t Current Score: 37.23\n",
      "Episode 103\tAverage Score: 20.17 \t Current Score: 37.02\n",
      "Episode 104\tAverage Score: 20.54 \t Current Score: 37.27\n",
      "Episode 105\tAverage Score: 20.90 \t Current Score: 36.26\n",
      "Episode 106\tAverage Score: 21.28 \t Current Score: 38.05\n",
      "Episode 107\tAverage Score: 21.66 \t Current Score: 38.16\n",
      "Episode 108\tAverage Score: 22.03 \t Current Score: 36.65\n",
      "Episode 109\tAverage Score: 22.38 \t Current Score: 35.28\n",
      "Episode 110\tAverage Score: 22.74 \t Current Score: 36.78\n",
      "Episode 111\tAverage Score: 23.09 \t Current Score: 34.49\n",
      "Episode 112\tAverage Score: 23.44 \t Current Score: 35.37\n",
      "Episode 113\tAverage Score: 23.80 \t Current Score: 36.56\n",
      "Episode 114\tAverage Score: 24.18 \t Current Score: 37.84\n",
      "Episode 115\tAverage Score: 24.54 \t Current Score: 36.27\n",
      "Episode 116\tAverage Score: 24.90 \t Current Score: 35.59\n",
      "Episode 117\tAverage Score: 25.25 \t Current Score: 35.61\n",
      "Episode 118\tAverage Score: 25.62 \t Current Score: 36.59\n",
      "Episode 119\tAverage Score: 25.98 \t Current Score: 36.29\n",
      "Episode 120\tAverage Score: 26.35 \t Current Score: 37.34\n",
      "Episode 121\tAverage Score: 26.71 \t Current Score: 37.17\n",
      "Episode 122\tAverage Score: 27.06 \t Current Score: 35.39\n",
      "Episode 123\tAverage Score: 27.41 \t Current Score: 36.11\n",
      "Episode 124\tAverage Score: 27.75 \t Current Score: 35.11\n",
      "Episode 125\tAverage Score: 28.08 \t Current Score: 34.42\n",
      "Episode 126\tAverage Score: 28.40 \t Current Score: 34.14\n",
      "Episode 127\tAverage Score: 28.73 \t Current Score: 35.03\n",
      "Episode 128\tAverage Score: 29.06 \t Current Score: 35.95\n",
      "Episode 129\tAverage Score: 29.38 \t Current Score: 34.90\n",
      "Episode 130\tAverage Score: 29.69 \t Current Score: 33.75\n",
      "Episode 131\tAverage Score: 30.00 \t Current Score: 35.05\n",
      "Episode 132\tAverage Score: 30.31 \t Current Score: 36.14\n",
      "\n",
      "Environment solved in 32 episodes!\tAverage Score: 30.31\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session # Udacity workspace utility functions\n",
    "\n",
    "# run the training loop\n",
    "with active_session():\n",
    "    scores, avgs = ddpg(n_episodes=500, max_t=1000, start_steps = 20, learn_frequency = 20, learn_count = 10, random_seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scores returned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde7c19a390>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='Score')\n",
    "plt.plot(np.arange(len(scores)), avgs, c='r', label='moving avg by last 100')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the environment once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Command-Line usage parser\n",
    "The following is parser for the command line interface for the DDPG solving the unity environment. \n",
    "<b>Not to be executed</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_episodes', type=int, default=500)\n",
    "    parser.add_argument('--max_t', type=int, default=1000)\n",
    "    parser.add_argument('--start_steps', type=int, default=20)\n",
    "    parser.add_argument('--learn_frequency', type=int, default=20)\n",
    "    parser.add_argument('--learn_count', type=int, default=10)\n",
    "    parser.add_argument('--random_seed', type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    ddpg(n_episodes = args.n_episodes, max_t = args.max_t, start_steps = args.start_steps, \\\n",
    "         learn_frequency = args.learn_frequency, learn_count = args.learn_count, random_seed = args.random_seed)\n",
    "env.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate script from the notebook\n",
    "Throughout this notebook you've been seeing cells marked with ```#export```. The script below picks cells that are marked and append them into python script. This is adopted from fast-ai's deep learning course practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Report.ipynb to nb_Report.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py Report.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, `nb_Report.py` script should have been generated in your workspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run the script from the command-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python nb_Report.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conclusions and Future Work\n",
    "\n",
    "#### Summary\n",
    "\n",
    "We've just learned to control double joints in Unity Environment, yay! Our DDPG agent proved to be versalite and able to solve the environment during repeated trials. This, by itself, proves that DDPG agent is able to solve such continuous control scenarios.\n",
    "\n",
    "Moreover, we manipulated 20 agents at the same time and solved the environment. I'm sure you can see how strong DDPG algorithm is. Its ability to deal with completely continuous setting makes it valuable in real-world scenario the **robotics**. In real-world, everything is continuous, therefore DDPG is something to try on.\n",
    "\n",
    "And, as always, you are most welcome to play around with the hyperparameters to make it even more efficient.\n",
    "\n",
    "### Future work\n",
    "\n",
    "This was DDPG agent with slight hyperparameters and training modifications. There are two ways we can go from here. First of all, we could try to solve even harder environment or even better, try the algorithm with robots! While I don't have any robots that I could use, I will direct you to harder Unity enviroment, [the Crawler](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#crawler). You are encouraged to follow their setup guide and tryout the agent we used here.\n",
    "\n",
    "Secondly, while DDPG can achieve great performance sometimes, it is frequently brittle with respect to hyperparameters and other kinds of tuning. A common failure mode for DDPG is that the learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function. There have been numerous improvements since 2015 when DDPG was first introduced. I encourage you to check out [(TD3)Twin Delayed DDPG](https://arxiv.org/abs/1802.09477) and [(D4PG) Distributed Distributional Deterministic Policy Gradients](https://arxiv.org/abs/1804.08617).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
